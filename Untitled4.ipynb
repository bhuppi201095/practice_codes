{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "name": ""
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "########################### Cell Giving Correct Output #########################\n\n###################### Development Cell #########################\n\n\n# Import necessary libraries for PySpark, creating a variable storing script - pyspark_code\n# Configure Spark session\n\npyspark_main_code_txt = ''\npyspark_code = ''\n\nfor pattern, match in ordered_statements:\n#Create Schema Pattern\n#######################################################################################################################################################\n#######################################################################################################################################################\n#######################################################################################################################################################\n#######################################################################################################################################################\n    if pattern == create_schema_pattern:\n        pass\n\n#Create Table Pattern\n#######################################################################################################################################################\n#######################################################################################################################################################\n#######################################################################################################################################################\n#######################################################################################################################################################    \n    elif pattern == create_table_pattern:\n        # Define SQL to PySpark data type mappings (case-insensitive)\n        sql_to_pyspark_datatypes = {\n            'INTEGER': 'IntegerType()',\n            'INT': 'IntegerType()',\n            'INT4': 'IntegerType()',\n            'INT8': 'LongType()',\n            'SMALLINT': 'ShortType()',\n            'BIGINT': 'LongType()',\n            'STRING': 'StringType()',\n            'VARCHAR': 'StringType()',\n            'CHAR': 'StringType()',\n            'DATE': 'DateType()',\n            'TIMESTAMP': 'TimestampType()',\n            'DECIMAL': 'DecimalType',\n            'NUMERIC': 'DecimalType',\n            'REAL': 'FloatType()',\n            'FLOAT': 'DoubleType()',\n            'DOUBLE PRECISION': 'DoubleType()',\n            'BOOLEAN': 'BooleanType()',\n            'BYTEA': 'BinaryType()' }\n            \n        # Process CREATE TABLE statements\n        schema_name = match.group(1).split('.')[0]\n        table_name = match.group(1).split('.')[1]\n        column_details = match.group(2)\n        print(schema_name)\n        print(table_name)\n        print(column_details)\n        \n        columns = []\n        for column in column_details.split(','):\n            parts = column.strip().split(maxsplit=1)\n            if len(parts) == 2:\n                column_name, data_type = parts\n                # Convert data type to uppercase for case-insensitive matching\n                data_type_upper = data_type.upper()\n                # Check if the data type is in the mapping\n                if data_type_upper in sql_to_pyspark_datatypes:\n                    if data_type_upper in ['VARCHAR', 'CHAR']:\n                        varchar_length = re.search(r'\\d+', data_type)\n                        if varchar_length:\n                            columns.append((column_name.strip(), f'StringType(maxLength={varchar_length.group()})'))\n                        else:\n                            columns.append((column_name.strip(), 'StringType()'))\n                    elif data_type_upper in ['DECIMAL', 'NUMERIC']:\n                        # Extract precision and scale from the data type\n                        precision_scale = re.findall(r'\\d+', data_type)\n                        if len(precision_scale) == 2:\n                            precision, scale = precision_scale\n                            columns.append((column_name.strip(), f'DecimalType({precision}, {scale})'))\n                        else:\n                            columns.append((column_name.strip(), 'DecimalType(10, 2)'))  # Default precision and scale\n                    else:\n                        columns.append((column_name.strip(), sql_to_pyspark_datatypes[data_type_upper]))\n                else:\n                    # Handle unknown data types (defaulting to StringType())\n                    columns.append((column_name.strip(), 'StringType()'))\n            elif len(parts) == 1:\n                column_name = parts[0]\n                # Handle the case when only the column name is provided without a data type\n                # Defaulting to StringType() here, you might want to adjust this behavior\n                columns.append((column_name.strip(), 'StringType()'))\n        \n        pyspark_code += f\"{table_name} = StructType([\\n\"\n        for column_name, data_type in columns:\n            pyspark_code += f\"    StructField('{column_name}', {data_type}),\\n\"\n        \n        pyspark_code = pyspark_code.rstrip(\",\\n\") + \"\\n])\\n\\n\" \\\n                       f\"df_{table_name} = spark.createDataFrame([], {table_name})\\n\" \\\n                       f\"df_{table_name}.write.mode('overwrite').saveAsTable('{schema_name}.{table_name}')\\n\"\n        pyspark_main_code_txt += \"\\n\\n\\n\"\n        pyspark_main_code_txt += pyspark_code\n        pyspark_main_code_txt += \"\\n\\n\\n\"\n        print(pyspark_main_code_txt)\n\n    elif pattern == update_pattern:        \n        \n        # Process UPDATE TABLE statements\n        full_code=match.group(0)\n        update_table = match.group(1)\n        set_clause = match.group(2)\n        from_clause = match.group(3)\n        join_type = match.group(4)\n        join_table = match.group(5)\n        join_condition = match.group(6)\n        where_clause = match.group(7)\n        if from_clause:\n            split_index = from_clause.rfind(' ')\n            table_name = from_clause[:split_index]\n            alias = from_clause[split_index+1:]\n            from_clause_with_as = f'{table_name} as {alias}'\n        # print(full_code)\n        # print(\"Update Table:\", update_table+ '\\n')\n        # print(\"Set Clause:\", set_clause+ '\\n')\n        # if from_clause:\n        #     print(\"From Clause:\", from_clause+ '\\n')\n        # else:\n        #     print(\"No from clause found\"+ '\\n')\n        # if where_clause:\n        #     print(\"Where Clause:\", where_clause+ '\\n'+ '---------------------Next Update Statement------------------'+ '\\n')\n        # else:\n        #     print(\"No where clause found\"+ '\\n'+ '---------------------Next Update Statement------------------'+ '\\n')\n\n  \n        ########################Creating the merge statement####################\n        \n        if update_table and set_clause and from_clause and where_clause:\n            pyspark_main_code_txt=f\"\"\"\n        spark.sql(\\\"\\\"\\\"\nMERGE INTO {update_table}\nUSING {from_clause_with_as}\nON {where_clause}\nWHEN MATCHED THEN\nUPDATE SET\n{set_clause} \\\"\\\"\\\")\n        \\n\\n\"\"\"\n\n        else:\n            pyspark_main_code_txt=f\"\"\"\nspark.sql(\\\"\\\"\\\" {full_code} \\\"\\\"\\\")\n            \\n\\n\"\"\"\n        \n\n        pyspark_main_code_txt += \"\\n\\n\\n\"\n        print(pyspark_main_code_txt)\n\n        \n        \n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "############################### Cell giving incorrect output #################\n\n\n# Import necessary libraries for PySpark, creating a variable storing script - pyspark_code\n# Configure Spark session\n\npyspark_main_code_txt = ''\npyspark_code = ''\n\nfor pattern, match in ordered_statements:\n#Create Schema Pattern\n#######################################################################################################################################################\n#######################################################################################################################################################\n#######################################################################################################################################################\n#######################################################################################################################################################\n    if pattern == create_schema_pattern:\n        pass\n\n#Create Table Pattern\n#######################################################################################################################################################\n#######################################################################################################################################################\n#######################################################################################################################################################\n#######################################################################################################################################################    \n    elif pattern == create_table_pattern:\n        # Define SQL to PySpark data type mappings (case-insensitive)\n        sql_to_pyspark_datatypes = {\n            'INTEGER': 'IntegerType()',\n            'INT': 'IntegerType()',\n            'INT4': 'IntegerType()',\n            'INT8': 'LongType()',\n            'SMALLINT': 'ShortType()',\n            'BIGINT': 'LongType()',\n            'STRING': 'StringType()',\n            'VARCHAR': 'StringType()',\n            'CHAR': 'StringType()',\n            'DATE': 'DateType()',\n            'TIMESTAMP': 'TimestampType()',\n            'DECIMAL': 'DecimalType',\n            'NUMERIC': 'DecimalType',\n            'REAL': 'FloatType()',\n            'FLOAT': 'DoubleType()',\n            'DOUBLE PRECISION': 'DoubleType()',\n            'BOOLEAN': 'BooleanType()',\n            'BYTEA': 'BinaryType()' }\n            \n        # Process CREATE TABLE statements\n        schema_name = match.group(1).split('.')[0]\n        table_name = match.group(1).split('.')[1]\n        column_details = match.group(2)\n        \n        columns = []\n        for column in column_details.split(','):\n            parts = column.strip().split(maxsplit=1)\n            if len(parts) == 2:\n                column_name, data_type = parts\n                # Convert data type to uppercase for case-insensitive matching\n                data_type_upper = data_type.upper()\n                # Check if the data type is in the mapping\n                if data_type_upper in sql_to_pyspark_datatypes:\n                    if data_type_upper in ['VARCHAR', 'CHAR']:\n                        varchar_length = re.search(r'\\d+', data_type)\n                        if varchar_length:\n                            columns.append((column_name.strip(), f'StringType(maxLength={varchar_length.group()})'))\n                        else:\n                            columns.append((column_name.strip(), 'StringType()'))\n                    elif data_type_upper in ['DECIMAL', 'NUMERIC']:\n                        # Extract precision and scale from the data type\n                        precision_scale = re.findall(r'\\d+', data_type)\n                        if len(precision_scale) == 2:\n                            precision, scale = precision_scale\n                            columns.append((column_name.strip(), f'DecimalType({precision}, {scale})'))\n                        else:\n                            columns.append((column_name.strip(), 'DecimalType(10, 2)'))  # Default precision and scale\n                    else:\n                        columns.append((column_name.strip(), sql_to_pyspark_datatypes[data_type_upper]))\n                else:\n                    # Handle unknown data types (defaulting to StringType())\n                    columns.append((column_name.strip(), 'StringType()'))\n            elif len(parts) == 1:\n                column_name = parts[0]\n                # Handle the case when only the column name is provided without a data type\n                # Defaulting to StringType() here, you might want to adjust this behavior\n                columns.append((column_name.strip(), 'StringType()'))\n        \n        pyspark_code += f\"{table_name} = StructType([\\n\"\n        for column_name, data_type in columns:\n            pyspark_code += f\"    StructField('{column_name}', {data_type}),\\n\"\n        \n        pyspark_code = pyspark_code.rstrip(\",\\n\") + \"\\n])\\n\\n\" \\\n                       f\"df_{table_name} = spark.createDataFrame([], {table_name})\\n\" \\\n                       f\"df_{table_name}.write.mode('overwrite').saveAsTable('{schema_name}.{table_name}')\\n\"\n        pyspark_main_code_txt += \"\\n\\n\\n\"\n        pyspark_main_code_txt += pyspark_code\n        pyspark_main_code_txt += \"\\n\\n\\n\"\n    \n#create_table_as_pattern   \n#######################################################################################################################################################\n#######################################################################################################################################################\n#######################################################################################################################################################\n#######################################################################################################################################################\n    elif pattern == create_table_as_pattern:\n        temp_table_name = match.group(1)\n        select_query = match.group(2)\n        \n        # Generate Spark SQL code to create permanent table\n        spark_sql_code = f\"spark.sql(\\'''CREATE TABLE {temp_table_name} AS {select_query.strip()} \\''')\\n\\n\\n\\n\"\n        pyspark_main_code_txt += \"\\n\\n\\n\"\n        pyspark_main_code_txt += spark_sql_code\n        pyspark_main_code_txt += \"\\n\\n\\n\"\n        \n################################# Update Table Pattern###############################################\n\n    elif pattern == update_pattern:        \n        \n        # Process UPDATE TABLE statements\n        full_code=match.group(0)\n        update_table = match.group(1)\n        set_clause = match.group(2)\n        from_clause = match.group(3)\n        join_type = match.group(4)\n        join_table = match.group(5)\n        join_condition = match.group(6)\n        where_clause = match.group(7)\n        if from_clause:\n            split_index = from_clause.rfind(' ')\n            table_name = from_clause[:split_index]\n            alias = from_clause[split_index+1:]\n            from_clause_with_as = f'{table_name} as {alias}'\n        # print(full_code)\n        # print(\"Update Table:\", update_table+ '\\n')\n        # print(\"Set Clause:\", set_clause+ '\\n')\n        # if from_clause:\n        #     print(\"From Clause:\", from_clause+ '\\n')\n        # else:\n        #     print(\"No from clause found\"+ '\\n')\n        # if where_clause:\n        #     print(\"Where Clause:\", where_clause+ '\\n'+ '---------------------Next Update Statement------------------'+ '\\n')\n        # else:\n        #     print(\"No where clause found\"+ '\\n'+ '---------------------Next Update Statement------------------'+ '\\n')\n\n  \n        ########################Creating the merge statement####################\n        \n        if update_table and set_clause and from_clause and where_clause:\n            pyspark_main_code_txt=f\"\"\"\n        spark.sql(\\\"\\\"\\\"\nMERGE INTO {update_table}\nUSING {from_clause_with_as}\nON {where_clause}\nWHEN MATCHED THEN\nUPDATE SET\n{set_clause} \\\"\\\"\\\")\n        \\n\\n\"\"\"\n\n        else:\n            pyspark_main_code_txt=f\"\"\"\nspark.sql(\\\"\\\"\\\" {full_code} \\\"\\\"\\\")\n            \\n\\n\"\"\"\n        \n\n        pyspark_main_code_txt += \"\\n\\n\\n\"        \n    \n#create_temp_table_pattern            \n#######################################################################################################################################################\n#######################################################################################################################################################\n#######################################################################################################################################################\n#######################################################################################################################################################\n    elif pattern == create_temp_table_pattern:\n        temp_table_name = match.group(1)\n        select_query = match.group(2)\n\n        # Generate Spark SQL code to create or replace temporary view\n        # spark_sql_code = f'spark.sql(\\'''{select_query.strip()} \\''')\\n' \n        spark_sql_code = f\"spark.sql('''{select_query.strip()}\\n''')\"\n        spark_sql_code += f'.createOrReplaceTempView(\"{temp_table_name}\")'\n        pyspark_main_code_txt += \"\\n\\n\\n\"\n        pyspark_main_code_txt += spark_sql_code\n        pyspark_main_code_txt += \"\\n\\n\\n\"\n\n#insert_into_select_pattern            \n#######################################################################################################################################################\n#######################################################################################################################################################\n#######################################################################################################################################################\n#######################################################################################################################################################\n    elif pattern == insert_into_select_pattern:\n        # if True:\n        table_name = match.group(1).strip()\n        insert_columns = [col.strip() for col in match.group(2).split(',') if col.strip()] if match.group(2) else None\n        select_statement = match.group(3).strip()\n\n        select_columns = [col.strip() for col in re.findall(r'\\b\\w+\\b', select_statement)]\n        missing_columns = set(insert_columns) - set(select_columns) if insert_columns else set()\n\n        if missing_columns:\n            select_statement = select_statement.replace('FROM', ', ' + ', '.join([f\"NULL AS {col}\" for col in missing_columns]) + ' FROM', 1)\n\n        spark_sql_statement = (\n            f\"spark.sql('''{select_statement}''').createOrReplaceTempView('t0')\\n\"\n            \"df = spark.sql('select * from t0')\\n\"\n            f\"df.write.option('header', 'true').mode('Append').save('{table_name}')\\n\\n\"\n        )\n        pyspark_main_code_txt += \"\\n\\n\\n\"\n        pyspark_main_code_txt += spark_sql_statement\n        pyspark_main_code_txt += \"\\n\\n\\n\"\n\n    elif pattern in (delete_pattern_with_where,delete_pattern_without_where ):   \n        spark_sql_code = f\"spark.sql('''{match.group(0).strip()}\\n''')\"\n        pyspark_main_code_txt += \"\\n\\n\\n\"\n        pyspark_main_code_txt += spark_sql_code\n        pyspark_main_code_txt += \"\\n\\n\\n\"\n        \n    elif pattern == drop_table_if_exists_pattern:   \n        spark_sql_code = f\"spark.sql('''{match.group(0).strip()}\\n''')\"\n        pyspark_main_code_txt += \"\\n\\n\\n\"\n        pyspark_main_code_txt += spark_sql_code\n        pyspark_main_code_txt += \"\\n\\n\\n\"\n    \n    elif pattern == alter_table_pattern:\n        alter_prefix = \"\"\"spark.sql('''  ALTER TABLE {0} SET TBLPROPERTIES (\n'delta.minReaderVersion' = '2',\n'delta.minWriterVersion' = '5',\n'delta.columnMapping.mode' = 'name'\n)''')\"\"\".format(match.group(1))\n\n        pyspark_main_code_txt += \"\\n\\n\\n\"\n        pyspark_main_code_txt += alter_prefix   \n        pyspark_main_code_txt += \"\\n\\n\\n\"\n        spark_sql_code = f\"spark.sql('''{match.group(0).strip()}\\n''')\"\n        pyspark_main_code_txt += spark_sql_code\n        pyspark_main_code_txt += \"\\n\\n\\n\"\n\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}